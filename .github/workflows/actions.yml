# This workflow installs the latest version of Terraform CLI and configures the Terraform CLI configuration file
# with an API token for Terraform Cloud (app.terraform.io). On pull request events, this workflow will run
# `terraform init`, `terraform fmt`, and `terraform plan` (speculative plan via Terraform Cloud). On push events
# to the main branch, `terraform apply` will be executed.
#
# Documentation for `hashicorp/setup-terraform` is located here: https://github.com/hashicorp/setup-terraform
#
# To use this workflow, you will need to complete the following setup steps.
#
# 1. Create a `main.tf` file in the root of this repository with the `remote` backend and one or more resources defined.
#   Example `main.tf`:
#     # The configuration for the `remote` backend.
#     terraform {
#       backend "remote" {
#         # The name of your Terraform Cloud organization.
#         organization = "example-organization"
#
#         # The name of the Terraform Cloud workspace to store Terraform state files in.
#         workspaces {
#           name = "example-workspace"
#         }
#       }
#     }
#
#     # An example resource that does nothing.
#     resource "null_resource" "example" {
#       triggers = {
#         value = "A example resource that does nothing!"
#       }
#     }
#
#
# 2. Generate a Terraform Cloud user API token and store it as a GitHub secret (e.g. TF_API_TOKEN) on this repository.
#   Documentation:
#     - https://www.terraform.io/docs/cloud/users-teams-organizations/api-tokens.html
#     - https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets
#
# 3. Reference the GitHub secret in step using the `hashicorp/setup-terraform` GitHub Action.
#   Example:
#     - name: Setup Terraform
#       uses: hashicorp/setup-terraform@v1
#       with:
#         cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}

name: 'Terraform'

on: [push, pull_request]

jobs:
  # terraform:
  #   name: 'Terraform'
  #   if: github.event_name == 'pull_request'
  #   runs-on: ubuntu-latest

  #   # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
  #   defaults:
  #     run:
  #       shell: bash

  #   env:
  #     DOCKER_REPOSITORY: bashox
  #     FRONTEND_IMAGE_NAME: revolgy-frontend
  #     BACKEND_IMAGE_NAME: revolgy-frontend
  #     IMAGE_TAG: ${{ github.run_number }} 
  #   steps:
  #   # Checkout the repository to the GitHub Actions runner
  #   - name: Checkout
  #     uses: actions/checkout@v3

  #   - name: Configure AWS Credentials
  #     uses: aws-actions/configure-aws-credentials@v1
  #     with:
  #       aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #       aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #       aws-region: ${{ secrets.AWS_REGION}}

  #   # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
  #   - name: Setup Terraform
  #     uses: hashicorp/setup-terraform@v1
  #     with:
  #       cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
  #       # terraform_wrapper: false

  #   # Initialize a new or existing Terraform working directory by creating initial files, loading any remote state, downloading modules, etc.
  #   - name: Terraform Init
  #     id: init
  #     run: |
  #       terraform -chdir=./production init
  #       terraform -chdir=./staging init


  #   # Checks that all Terraform configuration files adhere to a canonical format
  #   - name: Terraform Format
  #     id: format
  #     run: |
  #       terraform -chdir=./production fmt -check 
  #       terraform -chdir=./staging fmt -check 

  #   # validating the terraform code for any syntax error
  #   - name: Terraform validate
  #     id: validate
  #     run: |
  #         terraform -chdir=./production validate 
  #         terraform -chdir=./staging validate


  #   # Generates an execution plan for Terraform
  #   - name: Terraform Plan
  #     if: github.event_name == 'pull_request'
  #     id: plan
  #     run: terraform -chdir=./production plan -no-color 

  #   - name: Terraform Plan Status
  #     if: steps.plan.outcome == 'failure'
  #     run: exit 1



      
  #   # Add a comment to pull requests with results
  #   - name: add-results-comment
  #     id: comment
  #     uses: actions/github-script@v6
  #     if: github.event_name == 'pull_request'
  #     env:
  #       PLAN: "terraform\n${{ steps.plan.outputs.stdout }}"
      

  #     with:
  #       github-token: ${{ secrets.GITHUB_TOKEN }}
  #       script: |
  #         const output = `#### Terraform Initialization ‚öôÔ∏è\`${{ steps.init.outcome }}\`
  #         #### Terraform Format  üñå\`${{ steps.format.outcome }}\`
  #         #### Terraform Validation ü§ñ\`${{ steps.validate.outcome }}\`
  #         #### Terraform Plan üìñ\`${{ steps.plan.outcome }}\`
      
  #         <details><summary>Show Plan Changes</summary>
          
  #         \`\`\`\n
  #         ${process.env.PLAN}
  #         \`\`\`
          
  #         </details>

      
          
  #         *Pusher: @${{ github.actor }}, Action: \`${{ github.event_name }}\`, Working Directory: \`${{ env.tf_actions_working_dir }}\`, Workflow: \`${{ github.workflow }}\`*`;
            
  #         github.rest.issues.createComment({
  #           issue_number: context.issue.number,
  #           owner: context.repo.owner,
  #           repo: context.repo.repo,
  #           body: output
  #         })

  #     # On push to main, build or change infrastructure according to Terraform configuration files
  #     # Note: It is recommended to set up a required "strict" status check in your repository for "Terraform Cloud". See the documentation on "strict" required status checks for more information: https://help.github.com/en/github/administering-a-repository/types-of-required-status-checks
  #   - name: Terraform Apply
  #     # env:
  #     #   AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  #     #   AWS_REGION: ${{ secrets.AWS_REGION }}
  #     #   AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  #     if: github.ref == 'refs/heads/master' && github.event_name == 'push'

  #     run: terraform -chdir=./production  apply -auto-approve


  # terratest:
  #   name: 'Terratest'
  #   runs-on: ubuntu-latest
  #   if: github.event_name == 'pull_request'
  #   needs: [terraform]

  #   # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
  #   defaults:
  #     run:
  #       shell: bash
    
  #   steps:
  #   # Checkout the repository to the GitHub Actions runner
  #   - name: Checkout
  #     uses: actions/checkout@v3

  #    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
  #   - name: Setup Terraform
  #     uses: hashicorp/setup-terraform@v1
  #     with:
  #       cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
  #       terraform_wrapper: false ## setting it false because it gives terratest assertion, problem
  

  #   - name: Terraform terratest
  #     uses: actions/setup-go@v3
      
  # # testing if the resources are properly tagged
  #   - name: Infra Testing
  #     id: terratest 
  #     run: |
  #       go mod init github.com/Bash-mocart/terratest
  #       go mod tidy -go=1.16 && go mod tidy -go=1.17
  #       go test -v -timeout 3000m

  lint-test-build:
    name: 'Lint. Test, Build'
    runs-on: ubuntu-latest
    if: github.event_name == 'push'


    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash
    
    env:
      DOCKER_REPOSITORY: bashox
      FRONTEND_IMAGE_NAME: revolgy-frontend
      BACKEND_IMAGE_NAME: revolgy-backend
      IMAGE_TAG: ${{ github.run_number }}
    
    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      uses: actions/checkout@v3

    - name: setting up node
      uses: actions/setup-node@v3
      with:
        node-version: 13.8.0
        

    - name: linting backend
      working-directory: ./backend
      run: |
          npm install
          npm run lint

    - name: linting frontend
      working-directory: ./frontend
      run: |
          npm install  
          npm run lint

    
    - name: testing backend
      working-directory: ./backend
      run: |
          npm run test
        
    
    - name: testing frontend
      working-directory: ./frontend
      run: |
          npm run test

    - name: formatting frontend
      working-directory: ./frontend
      run: |
          npm run format
    
       
    - name: format backend
      working-directory: ./backend
      run: |
          npm run format
    
    - name: Set up QEMU
      uses: docker/setup-qemu-action@v2
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to DockerHub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
    
    - name: Build Docker Image frontend
      working-directory: ./frontend
      run:
        docker build --tag ${{ env.DOCKER_REPOSITORY }}/${{ env.FRONTEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} --no-cache .
    
    - name: Build Docker Image backend
      working-directory: ./backend
      run:
        docker build --tag ${{ env.DOCKER_REPOSITORY }}/${{ env.BACKEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} --no-cache .

    - name: Run Trivy vulnerability scanner frontend
      uses: aquasecurity/trivy-action@master
      continue-on-error: true
      with:
        image-ref: 'docker.io/${{ env.DOCKER_REPOSITORY }}/${{ env.FRONTEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }}'
        format: 'table'
        exit-code: '1'
        ignore-unfixed: true
        vuln-type: 'os'
        severity: 'MEDIUM,HIGH,CRITICAL'

    
    - name: Run Trivy vulnerability scanner backend
      uses: aquasecurity/trivy-action@master
      continue-on-error: true
      with:
        image-ref: 'docker.io/${{ env.DOCKER_REPOSITORY }}/${{ env.BACKEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }}'
        format: 'table'
        exit-code: '1'
        ignore-unfixed: true
        vuln-type: 'os'
        severity: 'MEDIUM,HIGH,CRITICAL'
    
    - name: Push Image to Docker Hub
      run: |
        docker push ${{ env.DOCKER_REPOSITORY }}/${{ env.BACKEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }}
        docker push ${{ env.DOCKER_REPOSITORY }}/${{ env.FRONTEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }}
  

  deploy-staging:
    name: 'deploy-staging'
    environment: staging
    needs: [lint-test-build]
    runs-on: ubuntu-latest
 
    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash
    
    env:
      DOCKER_REPOSITORY: bashox
      FRONTEND_IMAGE_NAME: revolgy-frontend
      BACKEND_IMAGE_NAME: revolgy-backend
      IMAGE_TAG: ${{ github.run_number }}
      CLUSTER_NAME: eks
    
    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      # if: github.event_name == 'pull_request'
      uses: actions/checkout@v3

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION}}

    
    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
        terraform_wrapper: false

    - name: Terraform Init
      id: init
      run: |
        terraform -chdir=./staging init
     


    # Checks that all Terraform configuration files adhere to a canonical format
    - name: Terraform Format
      id: format
      run: |
        terraform -chdir=./staging fmt -check 

    # validating the terraform code for any syntax error
    - name: Terraform validate
      id: validate
      run: | 
          terraform -chdir=./staging validate
    
    - name: Terraform Apply
      run: |
          terraform -chdir=./staging apply -auto-approve
  
    - name: Deploying Application
      run: aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }}

    
    
    - name: Eksctl, kubectl, helm tool installer
      run: |
          chmod +x install-helm-eksctl-kubectl.sh
          ./install-helm-eksctl-kubectl.sh

    - name: Creating policy
      continue-on-error: true
      run: |
        curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/iam_policy.json
        aws iam create-policy \
          --policy-name AWSLoadBalancerControllerIAMPolicy \
          --policy-document file://iam-policy.json

 
        
    - name: Installing ALB Controller
      continue-on-error: true
      run: |
        eksctl utils associate-iam-oidc-provider \
          --region ${{ secrets.AWS_REGION}} \
          --cluster ${{ env.CLUSTER_NAME }} \
          --approve
        eksctl create iamserviceaccount \
          --cluster=${{ env.CLUSTER_NAME }} \
          --namespace=kube-system \
          --name=aws-load-balancer-controller \
          --attach-policy-arn=arn:aws:iam::${{ secrets.ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
          --override-existing-serviceaccounts \
          --approve
        kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
        helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=${{ env.CLUSTER_NAME }} --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller
    - name: Installing alb, prometheus, grafana
      run: | 
          chmod +x install-alb-prom-graf.sh
          ./install-alb-prom-graf.sh

    - name: deploying application
      run: |
          echo starting
          export DNS_NAME=$(terraform -chdir=./staging output -raw postgresdns)
          echo yes 
          echo $DNS_NAME
          cd helm
          helm upgrade -i revolgy revolgy \
            --set database.DNS=$DNS_NAME \
            --set frontend.containerImage=${{ env.DOCKER_REPOSITORY }}/${{ env.FRONTEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} \
            --set backend.container.Image=${{ env.DOCKER_REPOSITORY }}/${{ env.BACKEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} \
            --set backend.container.TYPEORM_CONNECTION=${{ secrets.TYPEORM_CONNECTION }} \
            --set backend.container.TYPEORM_MIGRATIONS_DIR=${{ secrets.TYPEORM_MIGRATIONS_DIR }} \
            --set backend.container.TYPEORM_MIGRATIONS=${{ secrets.TYPEORM_MIGRATIONS }} \
            --set backend.container.TYPEORM_PORT=${{ secrets.TYPEORM_PORT }} \
            --set backend.container.TYPEORM_PASSWORD=${{ secrets.TYPEORM_PASSWORD }} \
            --set backend.container.TYPEORM_USERNAME=${{ secrets.TYPEORM_USERNAME }} \
            --set backend.container.TYPEORM_DATABASE=${{ secrets.TYPEORM_DATABASE }} 

    - name: Run Kube-Bench to check cluster config
      continue-on-error: true
      run: |
        kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job-aks.yaml
        sleep 30s
        kubectl logs job.batch/kube-bench
        kubectl delete job.batch/kube-bench

  deploy-prod:
    name: 'deploy-production'
    environment: production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/master' && github.event_name == 'push'

    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest
    defaults:
      run:
        shell: bash
    
    env:
      DOCKER_REPOSITORY: bashox
      FRONTEND_IMAGE_NAME: revolgy-frontend
      BACKEND_IMAGE_NAME: revolgy-backend
      IMAGE_TAG: ${{ github.run_number }}
    
    steps:
    # Checkout the repository to the GitHub Actions runner
    - name: Checkout
      # if: github.event_name == 'pull_request'
      uses: actions/checkout@v3

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION}}

    
    # Install the latest version of Terraform CLI and configure the Terraform CLI configuration file with a Terraform Cloud user API token
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v1
      with:
        cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }}
        terraform_wrapper: false

    - name: Terraform Init
      id: init
      run: |
        terraform -chdir=./production init
     


    # Checks that all Terraform configuration files adhere to a canonical format
    - name: Terraform Format
      id: format
      run: |
        terraform -chdir=./production fmt -check 

    # validating the terraform code for any syntax error
    - name: Terraform validate
      id: validate
      run: | 
          terraform -chdir=./production validate
    
    - name: Terraform Apply
      run: |
          terraform -chdir=./production apply -auto-approve
  

    - name: Deploying Application
      run: aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }}

    - name: Eksctl, kubectl, helm tool installer
      run: |
          chmod +x install-helm-eksctl-kubectl.sh
          ./install-helm-eksctl-kubectl.sh

    - name: Installing ALB Controller
      continue-on-error: true
      run: |
        eksctl utils associate-iam-oidc-provider \
          --region ${{ secrets.AWS_REGION}} \
          --cluster ${{ env.CLUSTER_NAME }} \
          --approve
        curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/iam_policy.json
        aws iam create-policy \
          --policy-name AWSLoadBalancerControllerIAMPolicy \
          --policy-document file://iam-policy.json
        eksctl create iamserviceaccount \
          --cluster=${{ env.CLUSTER_NAME }} \
          --namespace=kube-system \
          --name=aws-load-balancer-controller \
          --attach-policy-arn=arn:aws:iam::${{ secrets.ACCOUNT_ID }}:policy/AWSLoadBalancerControllerIAMPolicy \
          --override-existing-serviceaccounts \
          --approve

    - name: Installing alb, prometheus, grafana
      run: | 
          chmod +x install-alb-prom-graf.sh
          ./install-alb-prom-graf.sh

    - name: Eksctl, kubectl, helm tool installer
      run: |
          chmod +x ./install-helm-eksctl-kubectl.sh
          ./install-helm-eksctl-kubectl.sh
    
    - name: deploying application
      run: |
          echo starting
          export DNS_NAME=$(terraform -chdir=./production output -raw postgresdns)
          cd helm
          helm upgrade -i revolgy revolgy \
            --set database.DNS=$DNS_NAME \
            --set frontend.containerImage=${{ env.DOCKER_REPOSITORY }}/${{ env.FRONTEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} \
            --set backend.container.Image=${{ env.DOCKER_REPOSITORY }}/${{ env.BACKEND_IMAGE_NAME }}:${{ env.IMAGE_TAG }} \
            --set backend.container.TYPEORM_CONNECTION=${{ secrets.TYPEORM_CONNECTION }} \
            --set backend.container.TYPEORM_MIGRATIONS_DIR=${{ secrets.TYPEORM_MIGRATIONS_DIR }} \
            --set backend.container.TYPEORM_MIGRATIONS=${{ secrets.TYPEORM_MIGRATIONS }} \
            --set backend.container.TYPEORM_PORT=${{ secrets.TYPEORM_PORT }} \
            --set backend.container.TYPEORM_PASSWORD=${{ secrets.TYPEORM_PASSWORD }} \
            --set backend.container.TYPEORM_USERNAME=${{ secrets.TYPEORM_USERNAME }} \
            --set backend.container.TYPEORM_DATABASE=${{ secrets.TYPEORM_DATABASE }} 

    - name: Run Kube-Bench to check cluster config
      continue-on-error: true
      run: |
        kubectl apply -f https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job-aks.yaml
        sleep 30s
        kubectl logs job.batch/kube-bench
        kubectl delete job.batch/kube-bench